{"openapi":"3.1.0","info":{"title":"Squad API","description":"\n# Squad API - Multi-Provider LLM Orchestration\n\n**Production-ready LLM orchestration with intelligent fallback, rate limiting, and observability.**\n\n##  Features\n\n-  **Multi-Provider Support:** Groq, Cerebras, Gemini, OpenRouter, Together AI\n-  **Intelligent Fallback:** 3-tier fallback chain with quality verification\n-  **Rate Limiting:** Per-provider rate limits with auto-throttling\n-  **Agent Routing:** 13 specialized agents (code, creative, debug, data, etc.)\n-  **Observability:** Prometheus metrics + Grafana dashboards\n-  **Hot-Reload:** Update configuration without restart\n-  **Conversation Context:** Multi-turn conversations with Redis storage\n\n##  Quick Start\n\n1. **Get API Keys:** Obtain at least one provider key (Groq recommended)\n2. **Configure:** Copy `.env.example` to `.env` and add your keys\n3. **Start:** Run `docker-compose up -d`\n4. **Test:** Make your first request to `/v1/agents/code`\n\n##  Resources\n\n- **Documentation:** [GitHub README](https://github.com/your-org/squad-api)\n- **Deployment:** [Deployment Runbook](docs/runbooks/deployment.md)\n- **Troubleshooting:** [Troubleshooting Guide](docs/runbooks/troubleshooting.md)\n- **Monitoring:** [Grafana Dashboards](http://localhost:3000)\n    ","contact":{"name":"Squad API Team","url":"https://github.com/your-org/squad-api","email":"support@squad-api.example.com"},"license":{"name":"MIT License","url":"https://opensource.org/licenses/MIT"},"version":"1.0.0"},"servers":[{"url":"http://localhost:8000","description":"Development server"},{"url":"https://api.squad-api.example.com","description":"Production server"}],"paths":{"/v1/agents/{agent_name}":{"post":{"tags":["agents"],"summary":"Execute Agent Request","description":"## Execute Agent Request\n\nRoute a request to a specialized LLM agent with intelligent fallback and rate limiting.\n\n### Available Agents\n\n| Agent | Best For | Example Use Case |\n|-------|----------|------------------|\n| `code` | Programming tasks | \"Write a Python function to parse JSON\" |\n| `creative` | Content creation | \"Write a marketing email\" |\n| `debug` | Troubleshooting | \"Why is my API returning 500 errors?\" |\n| `data` | Data analysis | \"Analyze this CSV and find trends\" |\n| `architect` | System design | \"Design a microservices architecture\" |\n| `pm` | Project planning | \"Create a sprint plan for this feature\" |\n| `analyst` | Business analysis | \"Analyze market trends for this product\" |\n| `tech-writer` | Documentation | \"Write API documentation for this endpoint\" |\n\n### Request Body\n\n```json\n{\n  \"prompt\": \"Write a Python function to calculate Fibonacci numbers\",\n  \"conversation_id\": \"optional-conversation-id\",\n  \"metadata\": {\n    \"user_id\": \"john@example.com\",\n    \"session\": \"abc123\"\n  }\n}\n```\n\n### Response Fields\n\n- **response** (string): LLM-generated response\n- **provider** (string): Provider used (groq, cerebras, gemini, etc.)\n- **model** (string): LLM model used\n- **conversation_id** (string): Conversation ID for multi-turn dialogs\n- **metadata** (object):\n  - **fallback_used** (bool): Whether fallback was triggered\n  - **processing_time_ms** (int): Response time in milliseconds\n  - **tokens_used** (int): Tokens consumed\n  - **rate_limit_remaining** (int): Remaining requests before rate limit\n\n### Multi-Turn Conversations\n\nUse the same `conversation_id` to maintain context across requests:\n\n```bash\n# First request\ncurl -X POST http://localhost:8000/v1/agents/code \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Write a hello world function\",\n    \"conversation_id\": \"conv-123\"\n  }'\n\n# Follow-up request (remembers previous context)\ncurl -X POST http://localhost:8000/v1/agents/code \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Now add error handling\",\n    \"conversation_id\": \"conv-123\"\n  }'\n```\n\n### Fallback Chain\n\nIf the primary provider fails (rate limit, timeout, error), the request automatically\nfalls back to alternative providers:\n\n1. **Primary:** Groq (fast, cost-effective)\n2. **Secondary:** Cerebras (high quality)\n3. **Tertiary:** Gemini (reliable fallback)\n\n### Rate Limiting\n\nRate limits are enforced per provider:\n- **Groq:** 60 requests/min, 60,000 tokens/min\n- **Cerebras:** 30 requests/min, 900,000 tokens/min\n- **Gemini:** 60 requests/min, 32,000 tokens/min\n\nAuto-throttling activates at 80% of limit to prevent hard failures.","operationId":"execute_agent_v1_agents__agent_name__post","parameters":[{"name":"agent_name","in":"path","required":true,"schema":{"type":"string","title":"Agent Name"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"$ref":"#/components/schemas/AgentExecutionRequest"}}}},"responses":{"200":{"description":"Successful agent execution","content":{"application/json":{"schema":{"$ref":"#/components/schemas/AgentExecutionResponse"},"example":{"response":"Here's a Python function to calculate Fibonacci numbers:\\n\\n```python\\ndef fibonacci(n):\\n    if n <= 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n```","provider":"groq","model":"llama-3.1-70b-versatile","conversation_id":"test-123","metadata":{"fallback_used":false,"processing_time_ms":234,"tokens_used":150,"rate_limit_remaining":45}}}}},"404":{"description":"Agent not found","content":{"application/json":{"example":{"detail":"Agent 'invalid-agent' not found. Available agents: code, creative, debug, data, ..."}}}},"429":{"description":"Rate limit exceeded","content":{"application/json":{"example":{"detail":"Rate limit exceeded for provider groq. Try again in 30 seconds."}}}},"500":{"description":"All providers failed","content":{"application/json":{"example":{"detail":"All providers failed. Last error: Timeout connecting to gemini"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/v1/agents":{"get":{"tags":["agents"],"summary":"List Available Agents","description":"## List All Available Agents\n\nReturns a comprehensive list of all available agents with their capabilities,\nspecializations, and metadata.\n\n### Response Fields\n\n- **count** (int): Total number of available agents\n- **agents** (array): List of agent objects\n  - **id** (string): Agent identifier (use this in `/v1/agents/{agent_name}`)\n  - **name** (string): Human-friendly agent name\n  - **description** (string): What the agent specializes in\n  - **capabilities** (array): List of agent capabilities\n  - **icon** (string): Visual icon for the agent\n\n### Agent Categories\n\n**Development Agents:**\n- `code` - Programming and code generation\n- `debug` - Troubleshooting and debugging\n- `architect` - System architecture and design\n- `tech-writer` - Technical documentation\n\n**Business Agents:**\n- `analyst` - Business analysis and research\n- `pm` - Project management and planning\n- `sm` - Scrum master and agile practices\n\n**Creative Agents:**\n- `creative` - Content creation and copywriting\n- `ux-designer` - User experience design\n\n**Data Agents:**\n- `data` - Data analysis and visualization\n- `tea` - Technical excellence architect\n\n### Example Usage\n\n```bash\ncurl http://localhost:8000/v1/agents\n```\n\nUse the returned agent IDs to make requests to `/v1/agents/{agent_name}`.","operationId":"list_agents_v1_agents_get","responses":{"200":{"description":"Successful response with agent list","content":{"application/json":{"schema":{},"example":{"count":13,"agents":[{"id":"code","name":"CodeMaster","description":"Expert in programming, debugging, and code review","capabilities":["Python","JavaScript","Java","Code Review","Debugging"],"icon":""},{"id":"creative","name":"CreativeGenius","description":"Specialized in content creation and creative writing","capabilities":["Marketing Copy","Blog Posts","Social Media","Storytelling"],"icon":""},{"id":"analyst","name":"Mary","description":"Business analyst expert in market research and data analysis","capabilities":["Market Research","Data Analysis","Business Strategy"],"icon":""}]}}}}}}},"/providers/{provider_name}":{"get":{"tags":["providers"],"summary":"Get status of a specific provider","description":"Returns real-time status metrics for a single provider","operationId":"get_provider_status_providers__provider_name__get","parameters":[{"name":"provider_name","in":"path","required":true,"schema":{"type":"string","title":"Provider Name"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ProviderStatus"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/health":{"get":{"tags":["health"],"summary":"Health Check","description":"## Health Check Endpoint\n\nReturns the current health status of Squad API and its dependencies.\n\n**Use this endpoint to:**\n- Verify Squad API is running\n- Check service readiness\n- Monitor service availability (load balancer health checks)\n\n**Response Fields:**\n- `status`: \"healthy\" or \"degraded\"\n- `service`: Service name\n- `version`: Current API version\n- `dependencies`: Status of Redis, PostgreSQL, etc. (coming soon)\n\n**Example Response:**\n```json\n{\n  \"status\": \"healthy\",\n  \"service\": \"squad-api\",\n  \"version\": \"1.0.0\",\n  \"dependencies\": {\n    \"redis\": \"connected\",\n    \"postgres\": \"connected\"\n  }\n}\n```","operationId":"health_health_get","responses":{"200":{"description":"Service health status","content":{"application/json":{"schema":{}}}}}}}},"components":{"schemas":{"AgentExecutionRequest":{"properties":{"agent":{"anyOf":[{"type":"string","minLength":1},{"type":"null"}],"title":"Agent","description":"Agent identifier resolved from the router","examples":["analyst","code","creative"]},"user_id":{"anyOf":[{"type":"string","minLength":1},{"type":"null"}],"title":"User Id","description":"End-user identifier for conversation tracking","examples":["user-123","customer-42"]},"prompt":{"type":"string","maxLength":10000,"minLength":1,"title":"Prompt","description":"The task or question for the agent to process","examples":["Write a Python function to calculate Fibonacci numbers","Analyze the performance bottlenecks in this system","Create a marketing email for our new product launch"]},"conversation_id":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Conversation Id","description":"Optional conversation ID for multi-turn dialogs. Use the same ID to maintain context across requests.","examples":["conv-123","session-abc",null]},"metadata":{"type":"object","title":"Metadata","description":"Optional metadata for tracking and analytics","examples":[{"session":"abc123","user_id":"john@example.com"},{"source":"web-app","version":"1.2.0"}]},"max_tokens":{"anyOf":[{"type":"integer","maximum":100000.0,"minimum":1.0},{"type":"null"}],"title":"Max Tokens","description":"Maximum tokens in the response (provider-specific)","examples":[1000,2000,null]},"temperature":{"anyOf":[{"type":"number","maximum":2.0,"minimum":0.0},{"type":"null"}],"title":"Temperature","description":"Sampling temperature (0.0 = deterministic, 1.0 = creative)","examples":[0.7,0.0,1.0]}},"type":"object","required":["prompt"],"title":"AgentExecutionRequest","description":"Agent execution payload shared between FastAPI and the orchestrator.","examples":[{"conversation_id":"test-123","metadata":{"user_id":"john@example.com"},"prompt":"Write a Python function to calculate Fibonacci numbers"},{"conversation_id":"test-123","max_tokens":2000,"prompt":"Continue from the previous response and add error handling","temperature":0.5}]},"AgentExecutionResponse":{"properties":{"response":{"type":"string","title":"Response","description":"The agent's response content (generated text, code, or analysis)","examples":["Here's a Python function to calculate Fibonacci numbers:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```","Based on my analysis, the main performance bottleneck is database query optimization.","I've completed the code review. Here are my findings..."]},"provider":{"type":"string","title":"Provider","description":"The LLM provider that successfully handled this request","examples":["groq","cerebras","gemini","openrouter"]},"model":{"type":"string","title":"Model","description":"The specific model used by the provider","examples":["llama-3.1-70b-versatile","llama-3.3-70b","gemini-2.0-flash-exp","qwen-2.5-coder-32b"]},"agent":{"type":"string","title":"Agent","description":"The agent identifier that was executed","examples":["analyst","developer","reviewer"]},"agent_name":{"type":"string","title":"Agent Name","description":"The display name of the agent that was executed","examples":["Mary","John","Code Reviewer"]},"conversation_id":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Conversation Id","description":"Conversation ID for multi-turn dialogs (use this in subsequent requests to maintain context)","examples":["conv-123","session-abc",null]},"metadata":{"allOf":[{"$ref":"#/components/schemas/ExecutionMetadata"}],"description":"Detailed execution metadata including latency, tokens, and fallback information"}},"type":"object","required":["response","provider","model","agent","agent_name","metadata"],"title":"AgentExecutionResponse","description":"Agent Execution Response Model\n\nComplete response from agent execution including the generated content,\nprovider information, and detailed execution metadata.","examples":[{"conversation_id":"test-123","metadata":{"fallback_used":false,"latency_ms":1850,"provider_chain":["groq"],"request_id":"req-abc123","tokens_input":2500,"tokens_output":1200,"tool_calls_count":0,"turns":1},"model":"llama-3.1-70b-versatile","provider":"groq","response":"Here's a Python function to calculate Fibonacci numbers:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```"},{"conversation_id":"test-123","metadata":{"fallback_used":true,"latency_ms":3200,"provider_chain":["groq","cerebras"],"rate_limit_wait_ms":500,"request_id":"req-xyz789","tokens_input":3800,"tokens_output":800,"tool_calls_count":0,"turns":2},"model":"llama-3.3-70b","provider":"cerebras","response":"I've optimized the function with memoization for better performance."}]},"ExecutionMetadata":{"properties":{"request_id":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Request Id","description":"Unique identifier for this request (for tracing and debugging)","examples":["req-abc123","trace-xyz789"]},"latency_ms":{"type":"integer","minimum":0.0,"title":"Latency Ms","description":"Total end-to-end latency in milliseconds (includes all retries and fallbacks)","examples":[1850,3200,750]},"tokens_input":{"type":"integer","minimum":0.0,"title":"Tokens Input","description":"Total input tokens consumed (prompt + conversation context)","examples":[2500,1000,500]},"tokens_output":{"type":"integer","minimum":0.0,"title":"Tokens Output","description":"Total output tokens generated by the model","examples":[1200,800,300]},"fallback_used":{"type":"boolean","title":"Fallback Used","description":"Whether fallback to alternative provider was triggered due to rate limits or failures","default":false,"examples":[false,true]},"provider_chain":{"anyOf":[{"items":{"type":"string"},"type":"array"},{"type":"null"}],"title":"Provider Chain","description":"Sequence of providers attempted (e.g., ['groq', 'cerebras'] if fallback occurred)","examples":[["groq"],["groq","cerebras"],["groq","cerebras","gemini"]]},"tool_calls_count":{"type":"integer","minimum":0.0,"title":"Tool Calls Count","description":"Number of tool calls made during execution (for agents with tool access)","default":0,"examples":[0,2,5]},"turns":{"type":"integer","minimum":1.0,"title":"Turns","description":"Number of conversation turns (1 for single request, >1 for multi-turn dialog)","default":1,"examples":[1,3,7]},"rate_limit_wait_ms":{"anyOf":[{"type":"integer","minimum":0.0},{"type":"null"}],"title":"Rate Limit Wait Ms","description":"Time spent waiting due to rate limiting (milliseconds)","examples":[null,500,2000]}},"type":"object","required":["latency_ms","tokens_input","tokens_output"],"title":"ExecutionMetadata","description":"Execution Metadata Model\n\nComprehensive metadata about the agent execution including performance metrics,\ntoken usage, and fallback information."},"HTTPValidationError":{"properties":{"detail":{"items":{"$ref":"#/components/schemas/ValidationError"},"type":"array","title":"Detail"}},"type":"object","title":"HTTPValidationError"},"ProviderStatus":{"properties":{"name":{"type":"string","title":"Name","description":"Provider identifier (groq, gemini, etc)"},"model":{"type":"string","title":"Model","description":"Model name"},"status":{"allOf":[{"$ref":"#/components/schemas/ProviderStatusEnum"}],"description":"Current status"},"rpm_limit":{"type":"integer","title":"Rpm Limit","description":"Rate limit (requests per minute)"},"rpm_current":{"type":"integer","title":"Rpm Current","description":"Current RPM usage"},"rpm_available":{"type":"integer","title":"Rpm Available","description":"RPM remaining"},"latency_avg_ms":{"type":"integer","title":"Latency Avg Ms","description":"Average response time (ms)","default":0},"latency_p95_ms":{"type":"integer","title":"Latency P95 Ms","description":"95th percentile latency (ms)","default":0},"total_requests":{"type":"integer","title":"Total Requests","description":"Total requests since startup","default":0},"total_failures":{"type":"integer","title":"Total Failures","description":"Total failed requests","default":0},"failure_rate":{"type":"number","title":"Failure Rate","description":"Failure rate 0.0-1.0","default":0.0},"last_error":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Last Error","description":"Last error message"},"last_error_time":{"anyOf":[{"type":"string","format":"date-time"},{"type":"null"}],"title":"Last Error Time","description":"Last error timestamp"},"last_429_time":{"anyOf":[{"type":"string","format":"date-time"},{"type":"null"}],"title":"Last 429 Time","description":"Last rate limit hit"},"last_request_time":{"anyOf":[{"type":"string","format":"date-time"},{"type":"null"}],"title":"Last Request Time","description":"Last successful request"},"enabled":{"type":"boolean","title":"Enabled","description":"Whether provider is enabled","default":true},"uptime_seconds":{"type":"integer","title":"Uptime Seconds","description":"Uptime in seconds","default":0}},"type":"object","required":["name","model","status","rpm_limit","rpm_current","rpm_available"],"title":"ProviderStatus","description":"Status of a single LLM provider."},"ProviderStatusEnum":{"type":"string","enum":["healthy","degraded","unavailable"],"title":"ProviderStatusEnum","description":"Provider status enumeration."},"ValidationError":{"properties":{"loc":{"items":{"anyOf":[{"type":"string"},{"type":"integer"}]},"type":"array","title":"Location"},"msg":{"type":"string","title":"Message"},"type":{"type":"string","title":"Error Type"}},"type":"object","required":["loc","msg","type"],"title":"ValidationError"}}},"tags":[{"name":"agents","description":"Agent execution endpoints - Route requests to specialized LLM agents"},{"name":"health","description":"Health check and system status endpoints"},{"name":"metrics","description":"Prometheus metrics endpoint for monitoring"}]}