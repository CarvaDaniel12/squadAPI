#!/usr/bin/env python3
"""
Squad Project Generator - End-to-End Test Suite
Story: Comprehensive testing of the complete project generation workflow

This script tests all components of the Squad Project Generator to ensure
they work together seamlessly and produce valid results.
"""

import asyncio
import json
import tempfile
import shutil
from pathlib import Path
import sys
import logging
from typing import Dict, Any

# Add the src directory to Python path for testing
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from generator.spec_parser import (
    parse_and_validate_specification,
    ProjectSpecValidator,
    ProjectSpecNormalizer,
    load_specification_template,
    ValidationResult
)
from generator.project_generator import ProjectGenerator, ProjectSpec, ProjectType
from generator.scaffolding import ProjectScaffolder, ProjectPackager
from generator.config_integration import SquadConfigIntegrator

# Set up logging for tests
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MockOrchestrator:
    """Mock orchestrator for testing without a real Squad instance"""

    async def execute(self, request):
        """Mock execution that returns sample responses"""
        class MockResponse:
            def __init__(self, content):
                self.response = content
                self.agent = request.agent
                self.provider = "mock"
                self.model = "mock-model"

        # Return mock responses based on agent type
        responses = {
            "mary": "# Project Architecture Plan\n\n## Overview\nThis project will be a modern web application with React frontend and Node.js backend.\n\n## Technical Architecture\n- Frontend: React with TypeScript\n- Backend: Node.js with Express\n- Database: PostgreSQL\n- Authentication: JWT\n\n## Component Structure\n1. User Management\n2. Dashboard\n3. Data Visualization\n4. API Integration\n\n## Security Considerations\n- Input validation\n- SQL injection prevention\n- XSS protection\n- CSRF tokens",

            "john": "import React, { useState, useEffect } from 'react';\nimport './App.css';\n\nfunction App() {\n  const [data, setData] = useState([]);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    fetch('/api/data')\n      .then(response => response.json())\n      .then(data => {\n        setData(data);\n        setLoading(false);\n      })\n      .catch(error => {\n        console.error('Error fetching data:', error);\n        setLoading(false);\n      });\n  }, []);\n\n  if (loading) {\n    return <div className=\"loading\">Loading...</div>;\n  }\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <h1>Generated Application</h1>\n        <p>Welcome to your Squad-generated project!</p>\n      </header>\n      \n      <main className=\"main-content\">\n        <div className=\"data-display\">\n          <h2>Data Items: {data.length}</h2>\n          {data.map((item, index) => (\n            <div key={index} className=\"data-item\">\n              <h3>{item.title || 'Sample Item'}</h3>\n              <p>{item.description || 'Generated content'}</p>\n            </div>\n          ))}\n        </div>\n      </main>\n    </div>\n  );\n}\n\nexport default App;",

            "alex": "# Test Suite\n\nimport React from 'react';\nimport { render, screen, fireEvent, waitFor } from '@testing-library/react';\nimport App from '../App';\n\n// Mock fetch for API calls\nglobal.fetch = jest.fn(() =>\n  Promise.resolve({\n    json: () => Promise.resolve([\n      { id: 1, title: 'Test Item 1', description: 'Test Description 1' },\n      { id: 2, title: 'Test Item 2', description: 'Test Description 2' }\n    ])\n  })\n);\n\ndescribe('App Component', () => {\n  test('renders loading state initially', () => {\n    render(<App />);\n    expect(screen.getByText(/loading/i)).toBeInTheDocument();\n  });\n\n  test('renders data after loading', async () => {\n    render(<App />);\n    \n    await waitFor(() => {\n      expect(screen.getByText('Data Items: 2')).toBeInTheDocument();\n    });\n  });\n\n  test('displays test items correctly', async () => {\n    render(<App />);\n    \n    await waitFor(() => {\n      expect(screen.getByText('Test Item 1')).toBeInTheDocument();\n      expect(screen.getByText('Test Item 2')).toBeInTheDocument();\n    });\n  });\n});\n\n# Documentation\n\n# Project Documentation\n\n## Overview\nThis is a sample application generated by Squad API Project Generator.\n\n## Features\n- React frontend with modern hooks\n- RESTful API integration\n- Responsive design\n- Error handling\n- Loading states\n\n## Setup\n1. Install dependencies: `npm install`\n2. Set up environment variables\n3. Start development server: `npm run dev`\n\n## Testing\nRun tests with: `npm test`\n\n## Deployment\nSee DEPLOYMENT_GUIDE.md for deployment instructions."
        }

        return MockResponse(responses.get(request.agent, "Mock response"))


class TestRunner:
    """Test runner for Squad Project Generator components"""

    def __init__(self):
        self.temp_dir = None
        self.test_results = {}

    def setup_test_environment(self):
        """Set up temporary test environment"""
        self.temp_dir = tempfile.mkdtemp(prefix="squad_gen_test_")
        logger.info(f"Test environment created: {self.temp_dir}")

    def cleanup_test_environment(self):
        """Clean up test environment"""
        if self.temp_dir and Path(self.temp_dir).exists():
            shutil.rmtree(self.temp_dir)
            logger.info("Test environment cleaned up")

    async def test_specification_parsing(self) -> Dict[str, Any]:
        """Test specification parsing and validation"""
        logger.info("Testing specification parsing and validation...")

        try:
            # Test with sample specification
            spec_file = Path("samples/ecommerce_spec.yaml")
            if not spec_file.exists():
                return {"status": "skipped", "reason": "Sample file not found"}

            spec, validation_result = parse_and_validate_specification(spec_file)

            # Check validation results
            errors = validation_result.get_errors()
            warnings = validation_result.get_warnings()

            result = {
                "status": "passed" if validation_result.is_valid else "failed",
                "spec_parsed": bool(spec),
                "is_valid": validation_result.is_valid,
                "errors_count": len(errors),
                "warnings_count": len(warnings),
                "spec_fields": list(spec.keys()) if spec else []
            }

            if errors:
                result["errors"] = [f"{e.field}: {e.message}" for e in errors]
            if warnings:
                result["warnings"] = [f"{w.field}: {w.message}" for w in warnings]

            return result

        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def test_project_generation_workflow(self) -> Dict[str, Any]:
        """Test the complete project generation workflow"""
        logger.info("Testing project generation workflow...")

        try:
            # Set up mock orchestrator
            orchestrator = MockOrchestrator()

            # Create generator
            generator = ProjectGenerator(orchestrator, self.temp_dir)

            # Load and parse sample specification
            spec_file = Path("samples/user_api_spec.yaml")
            if not spec_file.exists():
                return {"status": "skipped", "reason": "Sample file not found"}

            spec_dict, validation_result = parse_and_validate_specification(spec_file)

            # Create ProjectSpec object
            project_spec = ProjectSpec(
                name=spec_dict['name'],
                description=spec_dict['description'],
                project_type=ProjectType(spec_dict['project_type']),
                requirements=spec_dict['requirements'],
                tech_stack=spec_dict['tech_stack'],
                features=spec_dict['features'],
                architecture=spec_dict.get('architecture'),
                constraints=spec_dict.get('constraints', []),
                target_audience=spec_dict.get('target_audience'),
                deployment_target=spec_dict.get('deployment_target', 'local')
            )

            # Generate project (this will use mock responses)
            result = await generator.generate_project(project_spec)

            return {
                "status": "passed" if result.status.value == "completed" else "failed",
                "project_id": result.project_id,
                "files_created": len(result.files_created),
                "generation_time": result.generation_time,
                "artifacts_path": result.artifacts_path,
                "errors": result.errors
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def test_scaffolding_system(self) -> Dict[str, Any]:
        """Test project scaffolding and file structure generation"""
        logger.info("Testing scaffolding system...")

        try:
            scaffolder = ProjectScaffolder()

            # Create a test project directory
            test_project_dir = Path(self.temp_dir) / "test_project"
            test_project_dir.mkdir()

            # Create a simple project spec
            spec = ProjectSpec(
                name="Test Project",
                description="A test project for scaffolding",
                project_type=ProjectType.WEB_APP,
                requirements=["Test requirement"],
                tech_stack=["react", "nodejs"],
                features=["Test feature"]
            )

            # Create project structure
            files_created = scaffolder.create_project_structure(
                test_project_dir,
                spec,
                {"frontend_development": "// Mock generated content"}
            )

            # Verify files were created
            expected_files = ["README.md", "package.json", "src/App.jsx"]
            created_file_names = [Path(f).name for f in files_created]

            matched_files = sum(1 for f in expected_files if any(f in cf for cf in created_file_names))

            return {
                "status": "passed" if len(files_created) > 5 else "failed",
                "files_created": len(files_created),
                "expected_files_matched": matched_files,
                "expected_files": expected_files,
                "sample_created_files": created_file_names[:5]
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def test_packaging_system(self) -> Dict[str, Any]:
        """Test project packaging and delivery"""
        logger.info("Testing packaging system...")

        try:
            # Create a simple test project directory
            test_project_dir = Path(self.temp_dir) / "packaging_test"
            test_project_dir.mkdir()

            # Create some test files
            (test_project_dir / "README.md").write_text("# Test Project\n\nGenerated by Squad API")
            (test_project_dir / "package.json").write_text('{"name": "test"}')

            # Test packaging
            packager = ProjectPackager()

            # Test ZIP packaging
            zip_path = packager.package_project(test_project_dir, 'zip')

            # Verify package was created
            zip_exists = zip_path.exists()
            zip_size = zip_path.stat().st_size if zip_exists else 0

            return {
                "status": "passed" if zip_exists and zip_size > 100 else "failed",
                "package_created": zip_exists,
                "package_size": zip_size,
                "package_path": str(zip_path)
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def test_template_system(self) -> Dict[str, Any]:
        """Test template loading and management"""
        logger.info("Testing template system...")

        try:
            # Test loading built-in templates
            templates = ['basic', 'web_app', 'api_service']
            loaded_templates = {}

            for template_name in templates:
                try:
                    template = load_specification_template(template_name)
                    loaded_templates[template_name] = {
                        "loaded": True,
                        "has_name": "name" in template,
                        "has_project_type": "project_type" in template,
                        "has_requirements": "requirements" in template
                    }
                except Exception as e:
                    loaded_templates[template_name] = {"loaded": False, "error": str(e)}

            # Check if all templates were loaded successfully
            successful_loads = sum(1 for t in loaded_templates.values() if t.get("loaded", False))

            return {
                "status": "passed" if successful_loads == len(templates) else "partial",
                "templates_tested": len(templates),
                "successful_loads": successful_loads,
                "template_details": loaded_templates
            }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all tests and compile results"""
        logger.info("Starting Squad Project Generator test suite...")

        self.setup_test_environment()

        try:
            tests = {
                "specification_parsing": self.test_specification_parsing(),
                "project_generation": self.test_project_generation_workflow(),
                "scaffolding": self.test_scaffolding_system(),
                "packaging": self.test_packaging_system(),
                "templates": self.test_template_system()
            }

            # Run all tests
            results = {}
            for test_name, test_coroutine in tests.items():
                logger.info(f"Running {test_name} test...")
                results[test_name] = await test_coroutine
                logger.info(f"{test_name} test completed: {results[test_name]['status']}")

            # Compile summary
            passed_tests = sum(1 for r in results.values() if r.get("status") == "passed")
            failed_tests = sum(1 for r in results.values() if r.get("status") == "failed")
            error_tests = sum(1 for r in results.values() if r.get("status") == "error")
            total_tests = len(results)

            summary = {
                "timestamp": str(Path().cwd()),
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "errors": error_tests,
                "success_rate": f"{(passed_tests/total_tests)*100:.1f}%" if total_tests > 0 else "0%",
                "overall_status": "passed" if failed_tests == 0 and error_tests == 0 else "partial" if passed_tests > 0 else "failed",
                "test_results": results
            }

            return summary

        finally:
            self.cleanup_test_environment()


async def main():
    """Main test execution function"""
    print("ğŸš€ Squad Project Generator - End-to-End Test Suite")
    print("=" * 60)

    # Create test runner
    runner = TestRunner()

    # Run tests
    results = await runner.run_all_tests()

    # Display results
    print("\nğŸ“Š Test Results Summary")
    print("-" * 30)
    print(f"Total Tests: {results['total_tests']}")
    print(f"âœ… Passed: {results['passed']}")
    print(f"âŒ Failed: {results['failed']}")
    print(f"ğŸ’¥ Errors: {results['errors']}")
    print(f"ğŸ“ˆ Success Rate: {results['success_rate']}")
    print(f"ğŸ¯ Overall Status: {results['overall_status'].upper()}")

    # Detailed results
    print("\nğŸ“‹ Detailed Results")
    print("-" * 30)
    for test_name, result in results['test_results'].items():
        status_emoji = {
            "passed": "âœ…",
            "failed": "âŒ",
            "error": "ğŸ’¥",
            "skipped": "â­ï¸"
        }.get(result.get("status"), "â“")

        print(f"{status_emoji} {test_name}: {result.get('status', 'unknown')}")

        # Show additional details for failed or error tests
        if result.get("status") in ["failed", "error"]:
            if "error" in result:
                print(f"   Error: {result['error']}")
            if "reason" in result:
                print(f"   Reason: {result['reason']}")

    # Save results to file
    results_file = Path("test_results.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nğŸ’¾ Detailed results saved to: {results_file}")

    # Exit with appropriate code
    if results['overall_status'] == "passed":
        print("\nğŸ‰ All tests passed! The Squad Project Generator is working correctly.")
        sys.exit(0)
    elif results['overall_status'] == "partial":
        print(f"\nâš ï¸ {results['failed'] + results['errors']} tests had issues. Check the details above.")
        sys.exit(1)
    else:
        print("\nğŸ’¥ Many tests failed. The Squad Project Generator needs attention.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
