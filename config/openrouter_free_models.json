[
  {
    "id": "google/gemini-2.0-flash-exp:free",
    "name": "Google: Gemini 2.0 Flash Experimental (free)",
    "context": 1048576,
    "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash "
  },
  {
    "id": "qwen/qwen3-coder:free",
    "name": "Qwen: Qwen3 Coder 480B A35B (free)",
    "context": 262000,
    "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the "
  },
  {
    "id": "kwaipilot/kat-coder-pro:free",
    "name": "Kwaipilot: KAT-Coder-Pro V1 (free)",
    "context": 256000,
    "description": "KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed s"
  },
  {
    "id": "tngtech/deepseek-r1t2-chimera:free",
    "name": "TNG: DeepSeek R1T2 Chimera (free)",
    "context": 163840,
    "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parame"
  },
  {
    "id": "deepseek/deepseek-r1-0528:free",
    "name": "DeepSeek: R1 0528 (free)",
    "context": 163840,
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI"
  },
  {
    "id": "tngtech/deepseek-r1t-chimera:free",
    "name": "TNG: DeepSeek R1T Chimera (free)",
    "context": 163840,
    "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoni"
  },
  {
    "id": "microsoft/mai-ds-r1:free",
    "name": "Microsoft: MAI DS R1 (free)",
    "context": 163840,
    "description": "MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the"
  },
  {
    "id": "deepseek/deepseek-chat-v3-0324:free",
    "name": "DeepSeek: DeepSeek V3 0324 (free)",
    "context": 163840,
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship cha"
  },
  {
    "id": "deepseek/deepseek-r1:free",
    "name": "DeepSeek: R1 (free)",
    "context": 163840,
    "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with full"
  },
  {
    "id": "deepseek/deepseek-chat-v3.1:free",
    "name": "DeepSeek: DeepSeek V3.1 (free)",
    "context": 163800,
    "description": "DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thi"
  },
  {
    "id": "alibaba/tongyi-deepresearch-30b-a3b:free",
    "name": "Tongyi DeepResearch 30B A3B (free)",
    "context": 131072,
    "description": "Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion tota"
  },
  {
    "id": "meituan/longcat-flash-chat:free",
    "name": "Meituan: LongCat Flash Chat (free)",
    "context": 131072,
    "description": "LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of wh"
  },
  {
    "id": "openai/gpt-oss-120b",
    "name": "OpenAI: gpt-oss-120b",
    "context": 131072,
    "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI d"
  },
  {
    "id": "openai/gpt-oss-20b:free",
    "name": "OpenAI: gpt-oss-20b (free)",
    "context": 131072,
    "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. I"
  },
  {
    "id": "z-ai/glm-4.5-air:free",
    "name": "Z.AI: GLM 4.5 Air (free)",
    "context": 131072,
    "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for a"
  },
  {
    "id": "mistralai/mistral-small-3.2-24b-instruct:free",
    "name": "Mistral: Mistral Small 3.2 24B (free)",
    "context": 131072,
    "description": "Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for ins"
  },
  {
    "id": "deepseek/deepseek-r1-0528-qwen3-8b:free",
    "name": "DeepSeek: DeepSeek R1 0528 Qwen3 8B (free)",
    "context": 131072,
    "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter pos"
  },
  {
    "id": "google/gemma-3-27b-it:free",
    "name": "Google: Gemma 3 27B (free)",
    "context": 131072,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont"
  },
  {
    "id": "meta-llama/llama-3.3-70b-instruct:free",
    "name": "Meta: Llama 3.3 70B Instruct (free)",
    "context": 131072,
    "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned gen"
  },
  {
    "id": "meta-llama/llama-3.2-3b-instruct:free",
    "name": "Meta: Llama 3.2 3B Instruct (free)",
    "context": 131072,
    "description": "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natu"
  },
  {
    "id": "nousresearch/hermes-3-llama-3.1-405b:free",
    "name": "Nous: Hermes 3 405B Instruct (free)",
    "context": 131072,
    "description": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced age"
  },
  {
    "id": "mistralai/mistral-nemo:free",
    "name": "Mistral: Mistral Nemo (free)",
    "context": 131072,
    "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA"
  },
  {
    "id": "nvidia/nemotron-nano-12b-v2-vl:free",
    "name": "NVIDIA: Nemotron Nano 12B 2 VL (free)",
    "context": 128000,
    "description": "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for vid"
  },
  {
    "id": "nvidia/nemotron-nano-9b-v2:free",
    "name": "NVIDIA: Nemotron Nano 9B V2 (free)",
    "context": 128000,
    "description": "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig"
  },
  {
    "id": "meta-llama/llama-3.3-8b-instruct:free",
    "name": "Meta: Llama 3.3 8B Instruct (free)",
    "context": 128000,
    "description": "A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed "
  },
  {
    "id": "meta-llama/llama-4-maverick:free",
    "name": "Meta: Llama 4 Maverick (free)",
    "context": 128000,
    "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built o"
  },
  {
    "id": "meta-llama/llama-4-scout:free",
    "name": "Meta: Llama 4 Scout (free)",
    "context": 128000,
    "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, act"
  },
  {
    "id": "agentica-org/deepcoder-14b-preview:free",
    "name": "Agentica: Deepcoder 14B Preview (free)",
    "context": 96000,
    "description": "DeepCoder-14B-Preview is a 14B parameter code generation model fine-tuned from DeepSeek-R1-Distill-Q"
  },
  {
    "id": "mistralai/mistral-small-3.1-24b-instruct:free",
    "name": "Mistral: Mistral Small 3.1 24B (free)",
    "context": 96000,
    "description": "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billio"
  },
  {
    "id": "qwen/qwen3-4b:free",
    "name": "Qwen: Qwen3 4B (free)",
    "context": 40960,
    "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support bo"
  },
  {
    "id": "qwen/qwen3-30b-a3b:free",
    "name": "Qwen: Qwen3 30B A3B (free)",
    "context": 40960,
    "description": "Qwen3, the latest generation in the Qwen large language model series, features both dense and mixtur"
  },
  {
    "id": "qwen/qwen3-14b:free",
    "name": "Qwen: Qwen3 14B (free)",
    "context": 40960,
    "description": "Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both "
  },
  {
    "id": "qwen/qwen3-235b-a22b:free",
    "name": "Qwen: Qwen3 235B A22B (free)",
    "context": 40960,
    "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B"
  },
  {
    "id": "moonshotai/kimi-k2:free",
    "name": "MoonshotAI: Kimi K2 0711 (free)",
    "context": 32768,
    "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, "
  },
  {
    "id": "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
    "name": "Venice: Uncensored (free)",
    "context": 32768,
    "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-In"
  },
  {
    "id": "arliai/qwq-32b-arliai-rpr-v1:free",
    "name": "ArliAI: QwQ 32B RpR v1 (free)",
    "context": 32768,
    "description": "QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative"
  },
  {
    "id": "google/gemma-3-4b-it:free",
    "name": "Google: Gemma 3 4B (free)",
    "context": 32768,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont"
  },
  {
    "id": "google/gemma-3-12b-it:free",
    "name": "Google: Gemma 3 12B (free)",
    "context": 32768,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles cont"
  },
  {
    "id": "mistralai/mistral-small-24b-instruct-2501:free",
    "name": "Mistral: Mistral Small 3 (free)",
    "context": 32768,
    "description": "Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across commo"
  },
  {
    "id": "qwen/qwen-2.5-coder-32b-instruct:free",
    "name": "Qwen2.5 Coder 32B Instruct (free)",
    "context": 32768,
    "description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as Co"
  },
  {
    "id": "qwen/qwen-2.5-72b-instruct:free",
    "name": "Qwen2.5 72B Instruct (free)",
    "context": 32768,
    "description": "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improve"
  },
  {
    "id": "mistralai/mistral-7b-instruct:free",
    "name": "Mistral: Mistral 7B Instruct (free)",
    "context": 32768,
    "description": "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context "
  },
  {
    "id": "qwen/qwen2.5-vl-32b-instruct:free",
    "name": "Qwen: Qwen2.5 VL 32B Instruct (free)",
    "context": 16384,
    "description": "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for e"
  },
  {
    "id": "google/gemma-3n-e2b-it:free",
    "name": "Google: Gemma 3n 2B (free)",
    "context": 8192,
    "description": "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to o"
  },
  {
    "id": "google/gemma-3n-e4b-it:free",
    "name": "Google: Gemma 3n 4B (free)",
    "context": 8192,
    "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as pho"
  },
  {
    "id": "deepseek/deepseek-r1-distill-llama-70b:free",
    "name": "DeepSeek: R1 Distill Llama 70B (free)",
    "context": 8192,
    "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct]("
  }
]