"""
Response Models
Story 1.7: Agent Execution Orchestrator
Epic 8: Enhanced OpenAPI Documentation
"""

from typing import Optional
from pydantic import BaseModel, Field


class ExecutionMetadata(BaseModel):
    """
    Execution Metadata Model

    Comprehensive metadata about the agent execution including performance metrics,
    token usage, and fallback information.
    """

    request_id: Optional[str] = Field(
        default=None,
        description="Unique identifier for this request (for tracing and debugging)",
        examples=["req-abc123", "trace-xyz789"]
    )

    latency_ms: int = Field(
        ...,
        description="Total end-to-end latency in milliseconds (includes all retries and fallbacks)",
        ge=0,
        examples=[1850, 3200, 750]
    )

    tokens_input: int = Field(
        ...,
        description="Total input tokens consumed (prompt + conversation context)",
        ge=0,
        examples=[2500, 1000, 500]
    )

    tokens_output: int = Field(
        ...,
        description="Total output tokens generated by the model",
        ge=0,
        examples=[1200, 800, 300]
    )

    fallback_used: bool = Field(
        default=False,
        description="Whether fallback to alternative provider was triggered due to rate limits or failures",
        examples=[False, True]
    )

    provider_chain: Optional[list[str]] = Field(
        default=None,
        description="Sequence of providers attempted (e.g., ['groq', 'cerebras'] if fallback occurred)",
        examples=[["groq"], ["groq", "cerebras"], ["groq", "cerebras", "gemini"]]
    )

    tool_calls_count: int = Field(
        default=0,
        description="Number of tool calls made during execution (for agents with tool access)",
        ge=0,
        examples=[0, 2, 5]
    )

    turns: int = Field(
        default=1,
        description="Number of conversation turns (1 for single request, >1 for multi-turn dialog)",
        ge=1,
        examples=[1, 3, 7]
    )

    rate_limit_wait_ms: Optional[int] = Field(
        default=None,
        description="Time spent waiting due to rate limiting (milliseconds)",
        ge=0,
        examples=[None, 500, 2000]
    )


class AgentExecutionResponse(BaseModel):
    """
    Agent Execution Response Model

    Complete response from agent execution including the generated content,
    provider information, and detailed execution metadata.
    """

    response: str = Field(
        ...,
        description="The agent's response content (generated text, code, or analysis)",
        examples=[
            "Here's a Python function to calculate Fibonacci numbers:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```",
            "Based on my analysis, the main performance bottleneck is database query optimization.",
            "I've completed the code review. Here are my findings..."
        ]
    )

    provider: str = Field(
        ...,
        description="The LLM provider that successfully handled this request",
        examples=["groq", "cerebras", "gemini", "openrouter"]
    )

    model: str = Field(
        ...,
        description="The specific model used by the provider",
        examples=[
            "llama-3.1-70b-versatile",
            "llama-3.3-70b",
            "gemini-2.0-flash-exp",
            "qwen-2.5-coder-32b"
        ]
    )

    agent: str = Field(
        ...,
        description="The agent identifier that was executed",
        examples=["analyst", "developer", "reviewer"]
    )

    agent_name: str = Field(
        ...,
        description="The display name of the agent that was executed",
        examples=["Mary", "John", "Code Reviewer"]
    )

    conversation_id: Optional[str] = Field(
        default=None,
        description="Conversation ID for multi-turn dialogs (use this in subsequent requests to maintain context)",
        examples=["conv-123", "session-abc", None]
    )

    metadata: ExecutionMetadata = Field(
        ...,
        description="Detailed execution metadata including latency, tokens, and fallback information"
    )

    class Config:
        json_schema_extra = {
            "examples": [
                {
                    "response": "Here's a Python function to calculate Fibonacci numbers:\n\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```",
                    "provider": "groq",
                    "model": "llama-3.1-70b-versatile",
                    "conversation_id": "test-123",
                    "metadata": {
                        "request_id": "req-abc123",
                        "latency_ms": 1850,
                        "tokens_input": 2500,
                        "tokens_output": 1200,
                        "fallback_used": False,
                        "provider_chain": ["groq"],
                        "tool_calls_count": 0,
                        "turns": 1,
                        "rate_limit_wait_ms": None
                    }
                },
                {
                    "response": "I've optimized the function with memoization for better performance.",
                    "provider": "cerebras",
                    "model": "llama-3.3-70b",
                    "conversation_id": "test-123",
                    "metadata": {
                        "request_id": "req-xyz789",
                        "latency_ms": 3200,
                        "tokens_input": 3800,
                        "tokens_output": 800,
                        "fallback_used": True,
                        "provider_chain": ["groq", "cerebras"],
                        "tool_calls_count": 0,
                        "turns": 2,
                        "rate_limit_wait_ms": 500
                    }
                }
            ]
        }

